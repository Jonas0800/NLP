{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "## Tokenizer",
   "id": "f9ff27442070a2eb"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "import re\n",
    "\n",
    "text = (\n",
    "    \"Bitcoin ist die erste und weltweit am Markt stärkste Kryptowährung. \"\n",
    "    \"Sie nutzt ein dezentrales Buchungssystem auf Basis einer Blockchain.[5][6] \"\n",
    "    \"Zahlungen werden kryptographisch legitimiert (digitale Signatur) und \"\n",
    "    \"über ein Rechnernetz gleichberechtigter Computer (Peer-to-Peer) abgewickelt. \"\n",
    "    \"Dr. Brown mag New York. Mr. Smith liebt (Digitale Kunst). \"\n",
    "    \"Er wurde am 19.2.2000 geboren.\"\n",
    ")"
   ],
   "id": "18a48b79a06013c8"
  },
  {
   "cell_type": "code",
   "id": "d77a449b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T19:18:10.434875Z",
     "start_time": "2025-07-18T19:18:10.431888Z"
    }
   },
   "source": [
    "def basic_tokenizer(text: str):\n",
    "    \"\"\"Zerlegt den Text mithilfe eines Regex in Roh‑Token.\"\"\"\n",
    "    pattern = r'\\[[^\\]]*\\]|\\([^)]*\\)|[A-Za-zÄÖÜäöüß0-9\\.]+|[^\\w\\s]'\n",
    "    return re.findall(pattern, text)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "6b49a10b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T19:18:12.188492Z",
     "start_time": "2025-07-18T19:18:12.184037Z"
    }
   },
   "source": [
    "def final_merge(tokens):\n",
    "    merged = []\n",
    "    i = 0\n",
    "    TITLES = {\"Dr.\", \"Mr.\", \"Mrs.\", \"Ms.\", \"Prof.\"}\n",
    "\n",
    "    while i < len(tokens):\n",
    "        tok = tokens[i]\n",
    "\n",
    "        # Titel + Name\n",
    "        if tok in TITLES and i + 1 < len(tokens) and tokens[i + 1].istitle():\n",
    "            merged.append(tok + \" \" + tokens[i + 1])\n",
    "            i += 2\n",
    "            continue\n",
    "\n",
    "        # Datum 19.2.2000  /  19 . 2 . 2000\n",
    "        if re.fullmatch(r\"\\d{1,2}\", tok):\n",
    "            \n",
    "            if (i + 2 < len(tokens)\n",
    "                and tokens[i + 1] == '.'\n",
    "                and re.fullmatch(r\"\\d{1,2}\", tokens[i + 2])):\n",
    "                \n",
    "                if (i + 4 < len(tokens)\n",
    "                    and tokens[i + 3] == '.'\n",
    "                    and re.fullmatch(r\"\\d{4}\", tokens[i + 4])):\n",
    "                    merged.append(f\"{tok}.{tokens[i + 2]}.{tokens[i + 4]}\")\n",
    "                    i += 5\n",
    "                    continue\n",
    "                elif re.fullmatch(r\"\\d{4}\", tokens[i + 2]):\n",
    "                    merged.append(f\"{tok}.{tokens[i + 2]}\")\n",
    "                    i += 3\n",
    "                    continue\n",
    "\n",
    "        # Aufeinanderfolgende Nomen\n",
    "        if tok.istitle() and i + 1 < len(tokens) and tokens[i + 1].istitle():\n",
    "            merged.append(tok + \" \" + tokens[i + 1])\n",
    "            i += 2\n",
    "            continue\n",
    "\n",
    "        merged.append(tok)\n",
    "        i += 1\n",
    "\n",
    "    return merged"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "619ccf8d",
   "metadata": {},
   "source": [
    "## Anwendung des Tokenizers auf den Beispieltext"
   ]
  },
  {
   "cell_type": "code",
   "id": "4c1919d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T14:54:22.573729Z",
     "start_time": "2025-06-22T14:54:22.569950Z"
    }
   },
   "source": [
    "raw_tokens = basic_tokenizer(text)\n",
    "final_tokens = final_merge(raw_tokens)\n",
    "print(\"Tokens nach basic_tokenizer (erste 40):\\n\", raw_tokens[:40], \"...\\n\")\n",
    "print(\"\\nFinale Tokens nach final_merge:\\n\", final_tokens)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens nach basic_tokenizer (erste 40):\n",
      " ['Bitcoin', 'ist', 'die', 'erste', 'und', 'weltweit', 'am', 'Markt', 'stärkste', 'Kryptowährung.', 'Sie', 'nutzt', 'ein', 'dezentrales', 'Buchungssystem', 'auf', 'Basis', 'einer', 'Blockchain.', '[5]', '[6]', 'Zahlungen', 'werden', 'kryptographisch', 'legitimiert', '(digitale Signatur)', 'und', 'über', 'ein', 'Rechnernetz', 'gleichberechtigter', 'Computer', '(Peer-to-Peer)', 'abgewickelt.', 'Dr.', 'Brown', 'mag', 'New', 'York.', 'Mr.'] ...\n",
      "\n",
      "\n",
      "Finale Tokens nach final_merge:\n",
      " ['Bitcoin', 'ist', 'die', 'erste', 'und', 'weltweit', 'am', 'Markt', 'stärkste', 'Kryptowährung. Sie', 'nutzt', 'ein', 'dezentrales', 'Buchungssystem', 'auf', 'Basis', 'einer', 'Blockchain.', '[5]', '[6]', 'Zahlungen', 'werden', 'kryptographisch', 'legitimiert', '(digitale Signatur)', 'und', 'über', 'ein', 'Rechnernetz', 'gleichberechtigter', 'Computer', '(Peer-to-Peer)', 'abgewickelt.', 'Dr. Brown', 'mag', 'New York.', 'Mr. Smith', 'liebt', '(Digitale Kunst)', '.', 'Er', 'wurde', 'am', '19.2.2000', 'geboren.']\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
